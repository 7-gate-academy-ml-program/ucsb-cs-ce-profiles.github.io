{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B42YV4QyuRKZ"
   },
   "source": [
    "# Instructions:\n",
    "## Please save a copy of this notebook to your google drive and answer the questions below.\n",
    "## Once completed please submit your notebook to the following [GitHub Repo](https://github.com/7-gate-academy-ml-program/Synopsis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d8mmOkNLyoWt"
   },
   "source": [
    "### Name: Kohei Suzuki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SdkchPgStYin"
   },
   "source": [
    "# 1.)  What is the difference between Classification and Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0aKWEz_atmEp"
   },
   "source": [
    "### Classification tasks\n",
    "The basic idea of classification tasks is to predict labels of given data. So basically, the outputs of classification tasks are discrete values which are corresponding to each class.\n",
    "As an example of common loss functions for classification tasks: **Cross Entropy Loss**\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "CrossEntropyLoss = -\\sum_{i=1}^{n}(y_i log(\\hat y_i) + (1-y_i)log(1-\\hat y_i))\n",
    "$$\n",
    "Where\n",
    "- $n$ is the number of data points\n",
    "- $y_i$ is the actual value of i th data point\n",
    "- $\\hat y_i$ is the predicted value for i th data point\n",
    "\n",
    "### Regression tasks\n",
    "The main focus of regression tasks is function approximation. So basically, the outputs of regression tasks are continuous values.\n",
    "As an example of common loss functions for regression tasks: **Mean Square Error**\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "MSE = \\frac{\\sum_{i=1}^{n}(y_i - \\hat y_i)^2}{n}\n",
    "$$\n",
    "Where\n",
    "- $n$ is the number of data points\n",
    "- $y_i$ is the actual value of i th data point\n",
    "- $\\hat y_i$ is the predicted value for i th data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ma7t_7yBtmud"
   },
   "source": [
    "# 2.) What is the Curse of Dimensionality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9C3_8rvyyDYA"
   },
   "source": [
    "To explain what the Curse of Demensionality is, suppose we have 3 features in our training set, and each feature has 5 kinds of categories.\n",
    "The table below is showing how feature spece grows up as the number of features increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|The number of features| Total feature space |\n",
    "|-|-|\n",
    "|1| 5 |\n",
    "|2|25|\n",
    "|3|125|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As the number of features increases, the more total feature space grows exponentially.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we tackle the Curse of Demensionality then?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Feature selection\n",
    "The idea of feature selection is to take a look at each feature we have in our training set and get rid of some features that we think less necessity than others.\n",
    "<br><br>\n",
    "2. Feature creation\n",
    "The idea of feature creation is to combine some features in our training set which are similar or sort of related to each other into one feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w_rFydWVyDl3"
   },
   "source": [
    "# 3.) What is Cross Validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using K-Folds Cross Validation generally results in a less biased model compare to other methods. Because every observation in our training set appears to both of training and test set.\n",
    "As an example of K-Folds Cross Validation, I created a table which shows how the cross validation works.\n",
    "I splited our training set into 5 chuncs, and `x` means that the chunc is being used as a training data and `o` means that the chunc is being used as a validation data.\n",
    "So let's take a look at the table now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-iA3jnCtxqP"
   },
   "source": [
    "|$\\quad$|chunk1|chunk2|chunk3|chunk4|chunk5|\n",
    "|-|-|-|-|-|-|\n",
    "|Iteration1|o|x|x|x|x|\n",
    "|Iteration2|x|o|x|x|x|\n",
    "|Iteration3|x|x|o|x|x|\n",
    "|Iteration4|x|x|x|o|x|\n",
    "|Iteration5|x|x|x|x|o|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the first iteration, we use `chunk2`, `chunk3`, `chunk4`, and `chunk5` as the training data, which means we train a model on them, and at the end of epoch (iteration1), we validate the model by using `chunk1` as validation data. \n",
    "Then iterate the process until every chunk is used as the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ieaSLJE2tyVm"
   },
   "source": [
    "# 4.) On a high level how do Decision  Trees work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sD6nBEWCt8ud"
   },
   "source": [
    "Decision trees build classification or regression models in the form of a tree structure. And the idea of the Decision trees is that we want to narrow down possibilities.\n",
    "The deeper the tree, the more complex the decision rules, and the fitter the model.\n",
    "At the first, A decision tree tries to find the best attribute in our dataset to split the dataset and to create the top if-else node, which is called the root node.\n",
    "After creating the node, it continues creating if-else nodes until it creates leaf nodes, which are the decisions that we should take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6akxVwD_t82G"
   },
   "source": [
    "# 5.) In regards to SVMs what is the Kernel Trick?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eDGR-lhHuKNq"
   },
   "source": [
    "Kernel trick is used when given $N$ dimensional data points are not linearly separable.\n",
    "So what we do in that case is to make the data points higher dimensions which may be able to linearly separatable.\n",
    "we need not compute the exact transformation of our data, we just need the inner product of our data in that higher-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "homework1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
